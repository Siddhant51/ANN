import numpy as np

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    return x * (1 - x)

# Input data
X = np.array([[0, 0, 1],
              [0, 1, 1],
              [1, 0, 1],
              [1, 1, 1]])

# Target outputs
y = np.array([[0],
              [1],
              [1],
              [0]])

# Randomly initialize weights
np.random.seed(1)
syn0 = 2 * np.random.random((3, 4)) - 1
syn1 = 2 * np.random.random((4, 1)) - 1

# Training
learning_rate = 0.1
iterations = 60000

for i in range(iterations):
    # Forward propagation
    layer0 = X
    layer1 = sigmoid(np.dot(layer0, syn0))
    layer2 = sigmoid(np.dot(layer1, syn1))

    # Backpropagation
    layer2_error = y - layer2
    layer2_delta = layer2_error * sigmoid_derivative(layer2)
    layer1_error = layer2_delta.dot(syn1.T)
    layer1_delta = layer1_error * sigmoid_derivative(layer1)

    # Update weights
    syn1 += learning_rate * layer1.T.dot(layer2_delta)
    syn0 += learning_rate * layer0.T.dot(layer1_delta)

    if i % 10000 == 0:
        print("Error: " + str(np.mean(np.abs(layer2_error))))
